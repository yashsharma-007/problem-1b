Sure! Here's your complete, **single block of markdown script** ready to paste directly into a `README.md` file on GitHub:

---

````markdown
# Document Section Extractor and Relevance Scorer

This project extracts sections from PDF documents and ranks them for relevance to a specified job role (e.g., _"Travel Planner planning a 4-day trip for 10 college friends"_) using semantic embeddings and TF-IDF scoring.

âœ… **Offline execution** | ğŸ’» **CPU-only** | ğŸ§  **Model size â‰¤ 1GB** | âš¡ **<60s for 3â€“5 PDFs**

---

## ğŸš€ Features

- **PDF Section Extraction**: Detects headings using:
  - Bold/italic font styles
  - Vertical spacing (>30 units)
  - 2â€“15 word titles (excluding dates)
- **Relevance Scoring**:
  - Semantic embeddings via `all-mpnet-base-v2` (ONNX, 0.85 weight)
  - TF-IDF scoring (0.15 weight)
  - TF-IDF-weighted keyword boosting for job alignment
- **Clean Output**:
  - JSON output (`output.json`) with ranked sections & relevant paragraphs
  - No extra formatting (e.g., bullets, newlines)

---

## ğŸ“¦ Prerequisites

- **Docker** (for containerized execution)
- **Python** 3.9+ (handled inside Docker)
- **Input Files**:
  - `input.json`: Includes persona, job-to-be-done, and list of PDFs
  - PDFs: Must be **text-based**, not scanned images

### âœ… Example `input.json`

```json
{
  "persona": { "role": "Travel Planner" },
  "job_to_be_done": { "task": "Plan a trip of 4 days for a group of 10 college friends." },
  "documents": [
    { "filename": "South of France - Cities.pdf" }
  ]
}
````

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/document-section-extractor.git
cd document-section-extractor
```

### 2. Prepare Input Files

* Place `input.json` and all PDF files into the `app/` directory.

### 3. Build Docker Image

```bash
docker build -t document-analyzer .
```

### 4. Run the Container

```bash
docker run --rm -v $(pwd)/app:/app document-analyzer
```

* Output will be in: `app/output.json`
* No internet required after setup

---

## ğŸ”„ Model Conversion (One-Time)

Convert `all-mpnet-base-v2` to ONNX using `optimum`:

```bash
pip install optimum
python script.py
```

* Copy the generated `model_onnx/` folder into `app/`

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ script.py                # Main script
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ Dockerfile               # Build instructions
â”œâ”€â”€ approach_explanation.md  # Technical details
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ input.json
â”‚   â”œâ”€â”€ *.pdf                # Input PDFs
â”‚   â”œâ”€â”€ model_onnx/          # Converted ONNX model
â”‚   â””â”€â”€ output.json          # Result file
```

---

## ğŸ³ Dockerfile

```Dockerfile
FROM python:3.9-slim

RUN apt-get update && apt-get install -y \
    libmupdf-dev \
    libfreetype6-dev \
    pkg-config \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

RUN python -m nltk.downloader punkt

COPY script.py .
COPY input.json .
COPY model_onnx/ /app/model_onnx/

CMD ["python", "script.py"]
```

---

## ğŸ“¦ requirements.txt

```txt
PyMuPDF==1.23.26
sentence-transformers==2.7.0
nltk==3.8.1
onnxruntime==1.17.1
numpy==1.26.4
scikit-learn==1.4.1.post1
optimum==1.17.1
```

---

## ğŸ§  Methodology

See `approach_explanation.md` for details on:

* Heading detection via font & spacing
* Semantic scoring using ONNX
* Keyword boosting based on job relevance
* Optimization techniques for speed and memory

---

## ğŸ› ï¸ Troubleshooting

| Issue                     | Solution                                                                                              |
| ------------------------- | ----------------------------------------------------------------------------------------------------- |
| **No sections extracted** | Verify PDF is text-based using:<br>`python -c "import fitz; print(fitz.open('file.pdf').get_text())"` |
| **Model errors**          | Ensure `model_onnx/` exists in `app/`                                                                 |
| **Slow processing**       | Reduce `max_length` in encoding; Limit extracted sections                                             |

---

## âš™ï¸ Constraints

* âœ… CPU-only (ONNX for fast inference)
* âœ… Model size < 1GB (`~420MB`)
* âœ… Processing time < 60s for 3â€“5 documents
* âœ… Fully offline after setup

---

## ğŸ“„ License

MIT License. See [LICENSE](LICENSE) for details.

```

---

Let me know if you want this as a downloadable `README.md` file or want help pushing it to your GitHub repo!
```
